# standard settings + BPE
base-ja-en: !Experiment
  exp_global: !ExpGlobal
    default_layer_dim: 1024
    dropout: 0.3
  preproc: !PreprocRunner
    overwrite: False
    tasks:
    - !PreprocTokenize
      in_files:
      - '{EXP_DIR}/../ja/train.ja'
      - '{EXP_DIR}/../ja/train.en'
      - '{EXP_DIR}/../ja/dev.kftt.ja'
      - '{EXP_DIR}/../ja/dev.kftt.en'
      - '{EXP_DIR}/../ja/dev.ja'
      - '{EXP_DIR}/../ja/dev.en'
      - '{EXP_DIR}/../ja/test.kftt.ja'
      - '{EXP_DIR}/../ja/test.kftt.en'
      - '{EXP_DIR}/../ja/test.ja'
      - '{EXP_DIR}/../ja/test.en'
      out_files:
      - '{EXP_DIR}/../ja/train.tok.ja'
      - '{EXP_DIR}/../ja/train.tok.en'
      - '{EXP_DIR}/../ja/dev.kftt.tok.ja'
      - '{EXP_DIR}/../ja/dev.kftt.tok.en'
      - '{EXP_DIR}/../ja/dev.tok.ja'
      - '{EXP_DIR}/../ja/dev.tok.en'
      - '{EXP_DIR}/../ja/test.kftt.tok.ja'
      - '{EXP_DIR}/../ja/test.kftt.tok.en'
      - '{EXP_DIR}/../ja/test.tok.ja'
      - '{EXP_DIR}/../ja/test.tok.en'
      specs:
      - filenum: all
        tokenizers:
        - !SentencepieceTokenizer
          # Replace <SENTENCEPIECE_SRC_PATH> with src/ directory of sentencepiece repository
          # The spm_* executables should be located at <SENTENCEPIECE_SRC_PATH>
          path: ''
          train_files:
           - '{EXP_DIR}/../ja/train.en,{EXP_DIR}/../ja/train.ja'
          vocab_size: 16000
          model_type: bpe
          model_prefix: '{EXP_DIR}/../ja/bpe.16000.shared'
    - !PreprocFilter
      in_files:
      - '{EXP_DIR}/../ja/train.tok.ja'
      - '{EXP_DIR}/../ja/train.tok.en'
      out_files:
      - '{EXP_DIR}/../ja/train.tok.filter.ja'
      - '{EXP_DIR}/../ja/train.tok.filter.en'
      specs:
      - type: length
        min: 1
        max: 80
  model: !DefaultTranslator
    src_reader: !PlainTextReader
      vocab: !Vocab
        vocab_file: '{EXP_DIR}/../ja/bpe.16000.shared.vocab'
        sentencepiece_vocab: True
    trg_reader: !PlainTextReader
      vocab: !Ref { path: model.src_reader.vocab }
    src_embedder: !SimpleWordEmbedder
      emb_dim: 512
    encoder: !BiLSTMSeqTransducer
      layers: 2
    attender: !MlpAttender {}
    trg_embedder: !DenseWordEmbedder    # Represent target words as a 40000x512 matrix
      emb_dim: 512
    decoder: !MlpSoftmaxDecoder
#      bridge: !LinearBridge {}          # Initialize the first state of the decoder with an affine transform of the last state of the encoder
      bridge: !CopyBridge {}            # Use CopyBridge because LineaBridge is F***ING BUGGED
      rnn_layer: !UniLSTMSeqTransducer  # Just your standard LSTM decoder
        layers: 2                       # With 2 layers
      mlp_layer: !MLP
        hidden_dim: !Ref {path: model.trg_embedder.emb_dim}
        output_projector: !Ref { path: model.trg_embedder }
        activation: 'identity'
      label_smoothing: 0.0
    inference: !SimpleInference
      search_strategy: !BeamSearch
        beam_size: 5
        len_norm: !PolynomialNormalization
          apply_during_search: true
          m: 1.0
      post_process: 'join-piece'
  train: !SimpleTrainingRegimen
    run_for_epochs: 20  # Run for at most 20 epochs
    dev_every: 1000000  # Evaluate every 1M sentences
    initial_patience: 2 # Run for at least 2 epochs without decreasing the learning rate
    patience: 1         # After there is no improvement for 1 epoch, decrease the learning rate
    lr_decay: 0.5       # Decay the learning rate by half whenever there is no improvement
    lr_decay_times: 2   # If there is still no improvement after decreasing the learning rate 2 times in a row, stop training
    trainer: !AdamTrainer
      alpha: 0.001
    batcher: !WordSrcBatcher
      avg_batch_size: 32
    src_file: '{EXP_DIR}/../ja/train.tok.filter.ja'
    trg_file: '{EXP_DIR}/../ja/train.tok.filter.en'
    dev_tasks:
      - !LossEvalTask
        src_file: '{EXP_DIR}/../ja/dev.kftt.tok.ja'
        ref_file: '{EXP_DIR}/../ja/dev.kftt.tok.en'
      - !AccuracyEvalTask
        eval_metrics: bleu
        src_file: '{EXP_DIR}/../ja/dev.kftt.tok.ja'
        ref_file: '{EXP_DIR}/../ja/dev.kftt.en'
        hyp_file: '{EXP_DIR}/hyp/{EXP}.kftt.dev'
      - !AccuracyEvalTask
        eval_metrics: bleu
        src_file: '{EXP_DIR}/../ja/dev.tok.ja'
        ref_file: '{EXP_DIR}/../ja/dev.en'
        hyp_file: '{EXP_DIR}/hyp/{EXP}.dev'
  evaluate:
    - !AccuracyEvalTask
      eval_metrics: bleu
      src_file: '{EXP_DIR}/../ja/dev.kftt.tok.ja'
      ref_file: '{EXP_DIR}/../ja/dev.kftt.en'
      hyp_file: '{EXP_DIR}/hyp/{EXP}.kftt.dev'
    - !AccuracyEvalTask
      eval_metrics: bleu
      src_file: '{EXP_DIR}/../ja/dev.tok.ja'
      ref_file: '{EXP_DIR}/../ja/dev.en'
      hyp_file: '{EXP_DIR}/hyp/{EXP}.dev'
    - !AccuracyEvalTask
      eval_metrics: bleu
      src_file: '{EXP_DIR}/../ja/test.kftt.tok.ja'
      ref_file: '{EXP_DIR}/../ja/test.kftt.en'
      hyp_file: '{EXP_DIR}/hyp/{EXP}.kftt.test'
    - !AccuracyEvalTask
      eval_metrics: bleu
      src_file: '{EXP_DIR}/../ja/test.tok.ja'
      ref_file: '{EXP_DIR}/../ja/test.en'
      hyp_file: '{EXP_DIR}/hyp/{EXP}.test'
