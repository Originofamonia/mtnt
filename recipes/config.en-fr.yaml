# English to french
base-en-fr: !Experiment
  exp_global: !ExpGlobal
    default_layer_dim: 1024 # Hidden layer size 1024 by default
    dropout: 0.3            # Dropout 0.2 by default
  preproc: !PreprocRunner
    overwrite: True         # Don't redo preprocessing if it's been done once before
    tasks:
    # Tokenize the english data with sentencepiece (using bpe)
    - !PreprocTokenize
      in_files:
      - '{EXP_DIR}/../fr/clean.en'
      - '{EXP_DIR}/../fr/dev.en'
      - '{EXP_DIR}/../fr/test.en'
      out_files:
      - '{EXP_DIR}/../fr/train.tok.16k.en'
      - '{EXP_DIR}/../fr/dev.tok.16k.en'
      - '{EXP_DIR}/../fr/test.tok.16k.en'
      specs:
      - filenum: all
        tokenizers:
        - !SentencepieceTokenizer
          path: ''
          train_files:
           - '{EXP_DIR}/../fr/clean.en'
          vocab_size: 16000
          model_type: bpe
          model_prefix: '{EXP_DIR}/en-fr.en.bpe.16k'
    # Tokenize the french data with sentencepiece (using bpe)
    - !PreprocTokenize
      in_files:
      - '{EXP_DIR}/../fr/clean.fr'
      - '{EXP_DIR}/../fr/dev.fr'
      - '{EXP_DIR}/../fr/test.fr'
      out_files:
      - '{EXP_DIR}/../fr/train.tok.16k.fr'
      - '{EXP_DIR}/../fr/dev.tok.16k.fr'
      - '{EXP_DIR}/../fr/test.tok.16k.fr'
      specs:
      - filenum: all
        tokenizers:
        - !SentencepieceTokenizer
          path: ''
          train_files:
           - '{EXP_DIR}/../fr/clean.fr'
          vocab_size: 16000
          model_type: bpe
          model_prefix: '{EXP_DIR}/en-fr.fr.bpe.16k'
    # Clean up AGAIN (after tokenization)
    - !PreprocFilter
      in_files:
      - '{EXP_DIR}/../fr/train.tok.16k.en'
      - '{EXP_DIR}/../fr/train.tok.16k.fr'
      out_files:
      - '{EXP_DIR}/../fr/train.tok.16k.cln.en'
      - '{EXP_DIR}/../fr/train.tok.16k.cln.fr'
      specs:
      - type: length
        min: 1
        max: 70
  model: !DefaultTranslator
    # Load vocabularies from sentencepiece files
    src_reader: !PlainTextReader
      vocab: !Vocab
        vocab_file: '{EXP_DIR}/en-fr.en.bpe.16k.vocab'
        sentencepiece_vocab: True
    trg_reader: !PlainTextReader
      vocab: !Vocab
        vocab_file: '{EXP_DIR}/en-fr.fr.bpe.16k.vocab'
        sentencepiece_vocab: True
    src_embedder: !SimpleWordEmbedder   # Embed source words as 512 dimensional vectors
      emb_dim: 512
    encoder: !BiLSTMSeqTransducer
      layers: 2
    attender: !MlpAttender {}
    trg_embedder: !DenseWordEmbedder    # Represent target words as a 8000x512 matrix
      emb_dim: 512
    decoder: !MlpSoftmaxDecoder
      bridge: !CopyBridge {}            # Initialize the first state of the decoder with an affine transform of the last state of the encoder
      rnn_layer: !UniLSTMSeqTransducer  # Just your standard LSTM decoder
        layers: 2                       # With 2 layers
      mlp_layer: !MLP
        output_projector: !Ref
          path: model.trg_embedder      # Tie the softmax output to the target word embeddings
        hidden_dim: !Ref
          path: model.trg_embedder.emb_dim
        activation: 'identity'
      label_smoothing: 0.1              # Smooth the output labels with the uniform distribution
    inference: !SimpleInference
      search_strategy: !BeamSearch
        beam_size: 5
        len_norm: !PolynomialNormalization
          apply_during_search: true
          m: 1.0
      post_process: 'join-piece'        # De-sentencepiece after generation
  train: !SimpleTrainingRegimen
    run_for_epochs: 20  # Run for at most 20 epochs
    initial_patience: 2 # Run for at least 2 epochs without decreasing the learning rate
    patience: 1         # After there is no improvement for 1 epoch, decrease the learning rate
    lr_decay: 0.5       # Decay the learning rate by half whenever there is no improvement
    lr_decay_times: 2   # If there is still no improvement after decreasing the learning rate 2 times in a row, stop training
    trainer: !AdamTrainer
      alpha: 0.001
    batcher: !WordSrcBatcher
      avg_batch_size: 64
    src_file: '{EXP_DIR}/../fr/train.tok.16k.cln.en'
    trg_file: '{EXP_DIR}/../fr/train.tok.16k.cln.fr'
    dev_tasks:
      # For BLEU, we shall compare against the detokenized output
      - !AccuracyEvalTask
        eval_metrics: bleu
        src_file: '{EXP_DIR}/../fr/dev.tok.16k.en'
        ref_file: '{EXP_DIR}/../fr/dev.fr'
        hyp_file: '{EXP_DIR}/hyp/{EXP}.dev.fr'
      # For NLL, we compare against the tokenized output
      - !LossEvalTask
        src_file: '{EXP_DIR}/../fr/dev.tok.16k.en'
        ref_file: '{EXP_DIR}/../fr/dev.tok.16k.fr'
  evaluate:
    - !AccuracyEvalTask
      eval_metrics: bleu
      src_file: '{EXP_DIR}/../fr/dev.tok.16k.en'
      ref_file: '{EXP_DIR}/../fr/dev.fr'
      hyp_file: '{EXP_DIR}/hyp/{EXP}.dev.fr'
    - !AccuracyEvalTask
      eval_metrics: bleu
      src_file: '{EXP_DIR}/../fr/test.tok.16k.en'
      ref_file: '{EXP_DIR}/../fr/test.fr'
      hyp_file: '{EXP_DIR}/hyp/{EXP}.test.fr'

